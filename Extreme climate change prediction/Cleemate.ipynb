{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gttGh5eKrCd7"
   },
   "source": [
    "# **Extreme Climate Change Prediction (CLEEMATE)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "So6kEk7gq3Zb"
   },
   "source": [
    "# Resource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "wvJTqetxeUpQ"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import OneHotEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "be8PbLqtpoDk"
   },
   "source": [
    "# Gathering Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CHBjzFX5mqAJ",
    "outputId": "ad7814cc-adb2-43aa-dc0f-a541a4587c79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         date      time  location  humidity  temperature  weather_code  \\\n",
      "0  2024-11-25  00:00:00    501191        88           26             4   \n",
      "1  2024-11-25  01:00:00    501191        81           27             4   \n",
      "2  2024-11-25  02:00:00    501191        73           29             4   \n",
      "3  2024-11-25  03:00:00    501191        66           31             4   \n",
      "4  2024-11-25  04:00:00    501191        63           31             4   \n",
      "\n",
      "   wind_speed  \n",
      "0           1  \n",
      "1           1  \n",
      "2           1  \n",
      "3           1  \n",
      "4           1  \n"
     ]
    }
   ],
   "source": [
    "# Define column names to assign\n",
    "column_names = ['location', 'time', 'deg_min', 'deg_max', 'hum_min', 'hum_max', 'humidity', 'temperature', 'weather_code', 'wind_direction', 'wind_speed']\n",
    "\n",
    "# List of the dataset filenames\n",
    "dataset_files = [\n",
    "    'kecamatanforecast-jakarta.csv',\n",
    "    'kecamatanforecast-jambi.csv',\n",
    "    'kecamatanforecast-jawatengah.csv',\n",
    "    'kecamatanforecast-jawatimur.csv',\n",
    "    'kecamatanforecast-sumut.csv'\n",
    "]\n",
    "\n",
    "# Read each dataset, assign column names, and drop unwanted columns\n",
    "datasets = []\n",
    "for file in dataset_files:\n",
    "    # Read the CSV file\n",
    "    df = pd.read_csv(file, sep=';', header=None)\n",
    "\n",
    "    # Assign column names\n",
    "    df.columns = column_names\n",
    "\n",
    "    # Split the 'time' column into 'date' and 'time'\n",
    "    df[['date', 'time']] = df['time'].str.split(' ', expand=True)\n",
    "\n",
    "    # Reorder the columns to have 'date' first, followed by 'time'\n",
    "    df = df[['date', 'time'] + [col for col in df.columns if col not in ['date', 'time']]]\n",
    "\n",
    "    # Drop unnecessary columns\n",
    "    df = df.drop(columns=['deg_min', 'deg_max', 'hum_min', 'hum_max', 'wind_direction'])\n",
    "\n",
    "    # Append the cleaned dataset to the list\n",
    "    datasets.append(df)\n",
    "\n",
    "# Combine all datasets into one\n",
    "combined_df = pd.concat(datasets, ignore_index=True)\n",
    "\n",
    "# Show the updated dataset (optional)\n",
    "print(combined_df.head())\n",
    "\n",
    "# Save the combined dataframe to a new file\n",
    "combined_df.to_csv('cleemate-dataset.csv', sep=';', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5WTBfaEIp-X9"
   },
   "source": [
    "# Merge Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hf6G28z4N1Te",
    "outputId": "89efed58-12bf-43d7-a693-e25536a1bd9b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             location     provinsi        date      time  humidity  \\\n",
      "0  Kota Jakarta Timur  DKI Jakarta  2024-11-25  00:00:00        88   \n",
      "1  Kota Jakarta Timur  DKI Jakarta  2024-11-25  01:00:00        81   \n",
      "2  Kota Jakarta Timur  DKI Jakarta  2024-11-25  02:00:00        73   \n",
      "3  Kota Jakarta Timur  DKI Jakarta  2024-11-25  03:00:00        66   \n",
      "4  Kota Jakarta Timur  DKI Jakarta  2024-11-25  04:00:00        63   \n",
      "\n",
      "   temperature  weather_code  wind_speed  \n",
      "0           26             4           1  \n",
      "1           27             4           1  \n",
      "2           29             4           1  \n",
      "3           31             4           1  \n",
      "4           31             4           1  \n"
     ]
    }
   ],
   "source": [
    "# Load the kecamatanforecast-jawatimur-fix dataset\n",
    "df_forecast = pd.read_csv('cleemate-dataset.csv', sep=';')\n",
    "\n",
    "# Load the kecamatan_geofeatures dataset\n",
    "df_geofeatures = pd.read_csv('kecamatan_geofeatures.csv', sep=';')\n",
    "\n",
    "# Merge the datasets on the 'location' column to get the corresponding 'kota' and 'provinsi' values\n",
    "merged_df = pd.merge(df_forecast, df_geofeatures[['location', 'kota', 'provinsi']], on='location', how='left')\n",
    "\n",
    "# Replace 'location' column with 'kota' column\n",
    "merged_df['location'] = merged_df['kota']\n",
    "\n",
    "# Drop the 'kota' column, as we no longer need it\n",
    "merged_df = merged_df.drop(columns=['kota'])\n",
    "\n",
    "# Rearrange columns so that 'provinsi' comes after 'location'\n",
    "columns = ['location', 'provinsi'] + [col for col in merged_df.columns if col not in ['location', 'provinsi']]\n",
    "merged_df = merged_df[columns]\n",
    "\n",
    "# Show the updated dataframe (first few rows)\n",
    "print(merged_df.head())\n",
    "\n",
    "# Optionally, save the updated dataframe to a new CSV file\n",
    "merged_df.to_csv('cleemate-dataset-fix.csv', sep=';', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oCO2Se4vXXzs",
    "outputId": "174ee976-d5e8-4d5e-d0d1-342087d0a949"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             location     provinsi        date      time  humidity  \\\n",
      "0  Kota Jakarta Timur  DKI Jakarta  2024-11-25  00:00:00        88   \n",
      "1  Kota Jakarta Timur  DKI Jakarta  2024-11-25  01:00:00        81   \n",
      "2  Kota Jakarta Timur  DKI Jakarta  2024-11-25  02:00:00        73   \n",
      "3  Kota Jakarta Timur  DKI Jakarta  2024-11-25  03:00:00        66   \n",
      "4  Kota Jakarta Timur  DKI Jakarta  2024-11-25  04:00:00        63   \n",
      "\n",
      "   temperature  weather_code  wind_speed        weather  \n",
      "0           26             4           1  Berawan Tebal  \n",
      "1           27             4           1  Berawan Tebal  \n",
      "2           29             4           1  Berawan Tebal  \n",
      "3           31             4           1  Berawan Tebal  \n",
      "4           31             4           1  Berawan Tebal  \n"
     ]
    }
   ],
   "source": [
    "# Load the kecamatanforecast-jawatimur-fix dataset\n",
    "df_forecast = pd.read_csv('cleemate-dataset-fix.csv', sep=';')\n",
    "\n",
    "# Load the weather dataset\n",
    "df_weather = pd.read_csv('weather.csv', sep=';')\n",
    "\n",
    "# Merge the datasets on the 'weather_code' column to get the corresponding 'weather' values\n",
    "merged_df = pd.merge(df_forecast, df_weather[['weather_code', 'weather']], on='weather_code', how='left')\n",
    "\n",
    "# Show the updated dataframe (first few rows)\n",
    "print(merged_df.head())\n",
    "\n",
    "# Optionally, save the updated dataframe to a new CSV file\n",
    "merged_df.to_csv('cleemate-dataset-fix.csv', sep=';', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCUGFDfvqdF1"
   },
   "source": [
    "# Preprocessing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-qNVhqu755GA"
   },
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df_forecast = pd.read_csv('cleemate-dataset-fix.csv', sep=';')\n",
    "\n",
    "# Create necessary columns first (before imputation)\n",
    "df_forecast['temp_change'] = df_forecast['temperature'].diff().fillna(0)\n",
    "df_forecast['wind_speed_change'] = df_forecast['wind_speed'].diff().fillna(0)\n",
    "df_forecast['humidity_change'] = df_forecast['humidity'].diff().fillna(0)\n",
    "\n",
    "# Create lag features for 1-day lookback (shift the values by 1)\n",
    "df_forecast['temp_previous_day'] = df_forecast['temperature'].shift(1)\n",
    "df_forecast['wind_speed_previous_day'] = df_forecast['wind_speed'].shift(1)\n",
    "df_forecast['humidity_previous_day'] = df_forecast['humidity'].shift(1)\n",
    "\n",
    "# Rolling averages over the past 5 columns for temperature, wind speed, and humidity\n",
    "df_forecast['rolling_temp'] = df_forecast['temperature'].rolling(window=5).mean()\n",
    "df_forecast['rolling_wind'] = df_forecast['wind_speed'].rolling(window=5).mean()\n",
    "df_forecast['rolling_humidity'] = df_forecast['humidity'].rolling(window=5).mean()\n",
    "\n",
    "# Handle missing values by imputing (using median or mean)\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "df_forecast[['humidity', 'temperature', 'wind_speed', 'temp_change', 'wind_speed_change',\n",
    "             'humidity_change', 'temp_previous_day', 'wind_speed_previous_day',\n",
    "             'humidity_previous_day', 'rolling_temp', 'rolling_wind', 'rolling_humidity']] = imputer.fit_transform(\n",
    "    df_forecast[['humidity', 'temperature', 'wind_speed', 'temp_change', 'wind_speed_change',\n",
    "                 'humidity_change', 'temp_previous_day', 'wind_speed_previous_day',\n",
    "                 'humidity_previous_day', 'rolling_temp', 'rolling_wind', 'rolling_humidity']])\n",
    "\n",
    "# Convert 'weather_code' into one-hot encoded features\n",
    "weather_encoder = OneHotEncoder(sparse_output=False)\n",
    "weather_code_encoded = weather_encoder.fit_transform(df_forecast[['weather_code']])\n",
    "\n",
    "# Create a DataFrame from the one-hot encoded features and join with the original dataframe\n",
    "weather_code_df = pd.DataFrame(weather_code_encoded, columns=weather_encoder.get_feature_names_out(['weather_code']))\n",
    "df_forecast = pd.concat([df_forecast, weather_code_df], axis=1)\n",
    "\n",
    "# Define extreme fluctuation (label = 1 for extreme change, 0 otherwise)\n",
    "df_forecast['extreme_fluctuation'] = ((df_forecast['temp_change'].abs() > 5) |\n",
    "                                      (df_forecast['wind_speed_change'].abs() > 10) |\n",
    "                                      (df_forecast['humidity_change'].abs() > 10)).astype(int)\n",
    "\n",
    "# Drop rows with missing data due to lag or rolling operations\n",
    "df_forecast = df_forecast.dropna(subset=['extreme_fluctuation'])\n",
    "\n",
    "# Features (X) and Target (y)\n",
    "X = df_forecast[['humidity', 'temperature', 'wind_speed', 'temp_change', 'wind_speed_change',\n",
    "                 'humidity_change', 'temp_previous_day', 'wind_speed_previous_day',\n",
    "                 'humidity_previous_day', 'rolling_temp', 'rolling_wind', 'rolling_humidity'] + list(weather_code_df.columns)]\n",
    "y = df_forecast['extreme_fluctuation']\n",
    "\n",
    "# Normalize the data\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Split into train and test datasets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Handle class imbalance using SMOTE (oversampling the minority class)\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQygp3xwqhvF"
   },
   "source": [
    "# Build and Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KNWNLaBrNTRT",
    "outputId": "f02b97a6-6dd5-402d-ec93-c403a618f8ac"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/input_layer.py:26: UserWarning: Argument `input_shape` is deprecated. Use `shape` instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 3ms/step - accuracy: 0.9485 - loss: 0.1210 - val_accuracy: 0.9862 - val_loss: 0.0279\n",
      "Epoch 2/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 6ms/step - accuracy: 0.9885 - loss: 0.0300 - val_accuracy: 0.9937 - val_loss: 0.0146\n",
      "Epoch 3/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3ms/step - accuracy: 0.9924 - loss: 0.0205 - val_accuracy: 0.9963 - val_loss: 0.0101\n",
      "Epoch 4/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 4ms/step - accuracy: 0.9942 - loss: 0.0153 - val_accuracy: 0.9926 - val_loss: 0.0145\n",
      "Epoch 5/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.9953 - loss: 0.0126 - val_accuracy: 0.9981 - val_loss: 0.0055\n",
      "Epoch 6/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9961 - loss: 0.0107 - val_accuracy: 0.9975 - val_loss: 0.0057\n",
      "Epoch 7/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - accuracy: 0.9962 - loss: 0.0104 - val_accuracy: 0.9962 - val_loss: 0.0088\n",
      "Epoch 8/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.9966 - loss: 0.0094 - val_accuracy: 0.9979 - val_loss: 0.0061\n",
      "Epoch 9/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 4ms/step - accuracy: 0.9969 - loss: 0.0086 - val_accuracy: 0.9968 - val_loss: 0.0082\n",
      "Epoch 10/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9971 - loss: 0.0080 - val_accuracy: 0.9978 - val_loss: 0.0052\n",
      "Epoch 11/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.9973 - loss: 0.0071 - val_accuracy: 0.9988 - val_loss: 0.0032\n",
      "Epoch 12/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9973 - loss: 0.0076 - val_accuracy: 0.9982 - val_loss: 0.0049\n",
      "Epoch 13/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.9975 - loss: 0.0071 - val_accuracy: 0.9990 - val_loss: 0.0031\n",
      "Epoch 14/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - accuracy: 0.9976 - loss: 0.0069 - val_accuracy: 0.9992 - val_loss: 0.0029\n",
      "Epoch 15/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9976 - loss: 0.0064 - val_accuracy: 0.9992 - val_loss: 0.0026\n",
      "Epoch 16/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.9979 - loss: 0.0062 - val_accuracy: 0.9989 - val_loss: 0.0029\n",
      "Epoch 17/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9979 - loss: 0.0061 - val_accuracy: 0.9990 - val_loss: 0.0028\n",
      "Epoch 18/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.9981 - loss: 0.0056 - val_accuracy: 0.9991 - val_loss: 0.0024\n",
      "Epoch 19/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9981 - loss: 0.0056 - val_accuracy: 0.9990 - val_loss: 0.0032\n",
      "Epoch 20/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0050 - val_accuracy: 0.9986 - val_loss: 0.0037\n",
      "Epoch 21/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.9981 - loss: 0.0055 - val_accuracy: 0.9982 - val_loss: 0.0039\n",
      "Epoch 22/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 4ms/step - accuracy: 0.9983 - loss: 0.0051 - val_accuracy: 0.9988 - val_loss: 0.0031\n",
      "Epoch 23/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0047 - val_accuracy: 0.9993 - val_loss: 0.0022\n",
      "Epoch 24/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.9983 - loss: 0.0050 - val_accuracy: 0.9989 - val_loss: 0.0033\n",
      "Epoch 25/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0046 - val_accuracy: 0.9995 - val_loss: 0.0019\n",
      "Epoch 26/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0044 - val_accuracy: 0.9991 - val_loss: 0.0019\n",
      "Epoch 27/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0044 - val_accuracy: 0.9997 - val_loss: 0.0011\n",
      "Epoch 28/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0042 - val_accuracy: 0.9988 - val_loss: 0.0033\n",
      "Epoch 29/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 4ms/step - accuracy: 0.9987 - loss: 0.0040 - val_accuracy: 0.9990 - val_loss: 0.0027\n",
      "Epoch 30/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9984 - loss: 0.0045 - val_accuracy: 0.9958 - val_loss: 0.0134\n",
      "Epoch 31/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.9985 - loss: 0.0042 - val_accuracy: 0.9996 - val_loss: 0.0019\n",
      "Epoch 32/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.9987 - loss: 0.0036 - val_accuracy: 0.9998 - val_loss: 0.0013\n",
      "Epoch 33/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 0.9992 - val_loss: 0.0021\n",
      "Epoch 34/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9986 - loss: 0.0044 - val_accuracy: 0.9990 - val_loss: 0.0026\n",
      "Epoch 35/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9986 - loss: 0.0041 - val_accuracy: 0.9990 - val_loss: 0.0025\n",
      "Epoch 36/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m23s\u001b[0m 4ms/step - accuracy: 0.9987 - loss: 0.0037 - val_accuracy: 0.9989 - val_loss: 0.0026\n",
      "Epoch 37/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0038 - val_accuracy: 0.9988 - val_loss: 0.0036\n",
      "Epoch 38/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 0.9987 - val_loss: 0.0033\n",
      "Epoch 39/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9988 - loss: 0.0036 - val_accuracy: 0.9995 - val_loss: 0.0017\n",
      "Epoch 40/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.9988 - loss: 0.0038 - val_accuracy: 0.9979 - val_loss: 0.0063\n",
      "Epoch 41/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0035 - val_accuracy: 0.9996 - val_loss: 0.0014\n",
      "Epoch 42/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 0.9993 - val_loss: 0.0017\n",
      "Epoch 43/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 0.9994 - val_loss: 0.0015\n",
      "Epoch 44/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0037 - val_accuracy: 0.9994 - val_loss: 0.0017\n",
      "Epoch 45/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0034 - val_accuracy: 0.9978 - val_loss: 0.0052\n",
      "Epoch 46/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0036 - val_accuracy: 0.9998 - val_loss: 8.6230e-04\n",
      "Epoch 47/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.9988 - loss: 0.0034 - val_accuracy: 0.9996 - val_loss: 0.0014\n",
      "Epoch 48/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 3ms/step - accuracy: 0.9989 - loss: 0.0032 - val_accuracy: 0.9993 - val_loss: 0.0022\n",
      "Epoch 49/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 3ms/step - accuracy: 0.9991 - loss: 0.0028 - val_accuracy: 0.9995 - val_loss: 0.0013\n",
      "Epoch 50/50\n",
      "\u001b[1m4089/4089\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - accuracy: 0.9991 - loss: 0.0029 - val_accuracy: 0.9992 - val_loss: 0.0026\n"
     ]
    }
   ],
   "source": [
    "# Build the model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.InputLayer(input_shape=(X_train_resampled.shape[1],)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.3),\n",
    "    tf.keras.layers.Dense(32, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')  # Binary classification (0 or 1)\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train_resampled, y_train_resampled, epochs=50, batch_size=64, validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "67I8IDBVqoSa"
   },
   "source": [
    "# Testing and Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XpC2mEYbBnlB",
    "outputId": "54dd5f67-161b-4bbb-9590-f1516149b5f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m2038/2038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.9993 - loss: 0.0025\n",
      "Test Accuracy: 99.92%\n",
      "\u001b[1m2038/2038\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 1ms/step\n",
      "Precision: 0.9948946339343906\n",
      "Recall: 0.9996725605762934\n",
      "F1 Score: 0.9972778745644599\n",
      "[[55996    47]\n",
      " [    3  9159]]\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "y_pred = (model.predict(X_test) > 0.5).astype(int)\n",
    "\n",
    "print(f\"Precision: {precision_score(y_test, y_pred)}\")\n",
    "print(f\"Recall: {recall_score(y_test, y_pred)}\")\n",
    "print(f\"F1 Score: {f1_score(y_test, y_pred)}\")\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JFE1ZO10VHCb",
    "outputId": "87049d54-2666-4b47-fd3d-620076625d3d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Save the model to the runtime (local file system)\n",
    "model.save('/content/cleemate-model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oz6sr_57wCVp",
    "outputId": "cac9403c-234d-4632-a832-b2c4c2e71538"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'flask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mflask\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Flask, request, jsonify\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mrequests\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'flask'"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import requests\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from datetime import datetime\n",
    "\n",
    "# Load your trained model\n",
    "model = tf.keras.models.load_model('cleemate-model.h5')\n",
    "\n",
    "# Initialize preprocessing tools (scaler, imputer, etc.)\n",
    "scaler = StandardScaler()\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "\n",
    "# Create Flask app\n",
    "app = Flask(__name__)\n",
    "\n",
    "def get_bmkg_weather_data(kode_wilayah):\n",
    "    # Define BMKG API URL with the kode_wilayah_tingkat_iv parameter\n",
    "    api_url = f'https://api.bmkg.go.id/publik/prakiraan-cuaca?adm4={kode_wilayah}'\n",
    "\n",
    "    # Make the GET request\n",
    "    response = requests.get(api_url)\n",
    "\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def preprocess_weather_data(weather_data):\n",
    "    # Extract relevant data from BMKG API response\n",
    "    # The API returns a nested structure, so you may need to adjust according to the actual data format\n",
    "\n",
    "    try:\n",
    "        # Assuming weather data contains these fields (adjust based on API structure)\n",
    "        temperature = weather_data['temperature']  # Adjust the key names as necessary\n",
    "        humidity = weather_data['humidity']\n",
    "        wind_speed = weather_data['wind_speed']\n",
    "\n",
    "        # Calculate other necessary features\n",
    "        temp_change = 0  # Calculate temperature change if needed\n",
    "        wind_speed_change = 0  # Calculate wind speed change\n",
    "        humidity_change = 0  # Calculate humidity change\n",
    "\n",
    "        # Example: create a DataFrame for the input features\n",
    "        features = pd.DataFrame({\n",
    "            'temperature': [temperature],\n",
    "            'humidity': [humidity],\n",
    "            'wind_speed': [wind_speed],\n",
    "            'temp_change': [temp_change],\n",
    "            'wind_speed_change': [wind_speed_change],\n",
    "            'humidity_change': [humidity_change]\n",
    "        })\n",
    "\n",
    "        return features\n",
    "    except KeyError:\n",
    "        return None  # In case the structure doesn't match\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    # Get user input from the request (JSON)\n",
    "    data = request.get_json()\n",
    "    location = data['location']\n",
    "    date_input = data['date']\n",
    "    kode_wilayah = data['kode_wilayah']  # Adding kode_wilayah for BMKG API\n",
    "\n",
    "    # Step 1: Preprocess the input data (date, location)\n",
    "    date = datetime.strptime(date_input, \"%Y-%m-%d\")\n",
    "    day_of_week = date.weekday()\n",
    "    month = date.month\n",
    "    year = date.year\n",
    "\n",
    "    # Step 2: Fetch weather data from BMKG API\n",
    "    weather_data = get_bmkg_weather_data(kode_wilayah)\n",
    "\n",
    "    if not weather_data:\n",
    "        return jsonify({'error': 'Failed to retrieve weather data'}), 400\n",
    "\n",
    "    # Step 3: Process the retrieved weather data\n",
    "    features = preprocess_weather_data(weather_data)\n",
    "\n",
    "    if features is None:\n",
    "        return jsonify({'error': 'Failed to process weather data'}), 400\n",
    "\n",
    "    # Add date-based features\n",
    "    features['day_of_week'] = day_of_week\n",
    "    features['month'] = month\n",
    "    features['year'] = year\n",
    "\n",
    "    # Step 4: Preprocess the features (scaling, imputing missing values)\n",
    "    features = imputer.transform(features)  # Impute missing values\n",
    "    features_scaled = scaler.transform(features)  # Scale the features\n",
    "\n",
    "    # Step 5: Make the prediction\n",
    "    prediction = model.predict(features_scaled)\n",
    "    predicted_class = (prediction > 0.5).astype(int)  # Convert to binary output (0 or 1)\n",
    "\n",
    "    # Step 6: Return the prediction as a JSON response\n",
    "    return jsonify({'prediction': int(predicted_class[0])})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "So6kEk7gq3Zb",
    "be8PbLqtpoDk",
    "5WTBfaEIp-X9",
    "CCUGFDfvqdF1",
    "yQygp3xwqhvF",
    "67I8IDBVqoSa"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
